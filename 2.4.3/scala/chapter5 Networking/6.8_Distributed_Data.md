# 5.8 分布式数据

Akka分布式数据在需要在Akka集群中的节点之间共享数据时非常有用。数据通过提供像API这样的键值存储的actor来访问。键是具有数据值的类型信息的唯一标识符。值为无冲突复制数据类型（CRDT）。

所有数据条目通过直接复制和基于gossip的方式传播到集群中的所有节点或具有特定角色的节点。您可以对读取和写入的一致性级别进行精细控制。

CRDT特性使得可以在没有协调的情况下从任何节点执行更新。来自不同节点的并行更新将通过所有数据类型必须提供的单调合并函数自动解决。状态变化总是收敛。提供了用于计数器，集合，映射和寄存器的几种有用的数据类型，您还可以实现自己的自定义数据类型。

它最终是一致的，旨在提供高读取和写入可用性（分区容差），具有低延迟。注意，在最终一致的系统中，读取可能返回过时的值。

>警告：此模块在Akka 2.4.0中引入时被标记为“实验性”。我们将根据用户的反馈继续改进此API，这意味着虽然我们尝试将不兼容的更改保持为最小，但维护版本的二进制兼容性保证不适用于akka.persistence软件包的内容。

## 5.8.1 使用复制器

`akka.cluster.ddata.Replicator` actor提供用于与数据交互的API。必须在群集中的每个节点或标记有特定角色的节点组中启动Replicator actor。它与具有在其他节点上运行的相同路径（无地址）的其他Replicator实例进行通信。为了方便起见，它可以与akka.cluster.ddata.DistributedData扩展一起使用。

具有状态WeaklyUp的群集成员（如果启用了该功能）将不会参与分布式数据，但这是应该可以在将来的版本中添加的内容。

下面是一个actor的例子，它将tick消息调度到自己，每个tick都从ORSet (observed-remove set)中添加或删除元素。它也订阅了这个的更改。

```scala
import java.util.concurrent.ThreadLocalRandom
import akka.actor.Actor
import akka.actor.ActorLogging
import akka.cluster.Cluster
import akka.cluster.ddata.DistributedData
import akka.cluster.ddata.ORSet
import akka.cluster.ddata.ORSetKey
import akka.cluster.ddata.Replicator
import akka.cluster.ddata.Replicator._
 
object DataBot {
  private case object Tick
}
 
class DataBot extends Actor with ActorLogging {
  import DataBot._
 
  val replicator = DistributedData(context.system).replicator
  implicit val node = Cluster(context.system)
 
  import context.dispatcher
  val tickTask = context.system.scheduler.schedule(5.seconds, 5.seconds, self, Tick)
 
  val DataKey = ORSetKey[String]("key")
 
  replicator ! Subscribe(DataKey, self)
 
  def receive = {
    case Tick =>
      val s = ThreadLocalRandom.current().nextInt(97, 123).toChar.toString
      if (ThreadLocalRandom.current().nextBoolean()) {
        // add
        log.info("Adding: {}", s)
        replicator ! Update(DataKey, ORSet.empty[String], WriteLocal)(_ + s)
      } else {
        // remove
        log.info("Removing: {}", s)
        replicator ! Update(DataKey, ORSet.empty[String], WriteLocal)(_ - s)
      }
 
    case _: UpdateResponse[_] => // ignore
 
    case c @ Changed(DataKey) =>
      val data = c.get(DataKey)
      log.info("Current elements: {}", data.elements)
  }
 
  override def postStop(): Unit = tickTask.cancel()
 
}
```
### 更新

要修改和复制数据值，请将Replicator.Update消息发送到本地复制器。 Update的键的当前数据值作为参数传递到Update的modify函数。该函数应该返回数据的新值，然后根据给定的一致性级别进行复制。修改函数由Replicator actor调用，因此必须是一个纯函数，它只使用来自封闭范围的数据参数和稳定字段。它必须例如不访问包含actor的sender（）引用。更新旨在仅从运行在相同本地ActorSystem中的actor发送

•复制器，因为修改功能通常不可序列化。

您提供写一致性级别，具有以下含义：
* `WriteLocal`		该值将立即只写入本地副本，并随后通过gossip传播
* `WriteTo(n)`		该值将立即写入至少n个副本，包括本地副本
* `WriteMajority`	该值将立即写入大多数副本，即至少N / 2 + 1个副本，其中N是集群（或集群角色组）中的节点数
* `WriteAll` 			将立即写入该值到集群中的所有节点（或集群角色组中的所有节点）

```scala
implicit val node = Cluster(system)
val replicator = DistributedData(system).replicator
 
val Counter1Key = PNCounterKey("counter1")
val Set1Key = GSetKey[String]("set1")
val Set2Key = ORSetKey[String]("set2")
val ActiveFlagKey = FlagKey("active")
 
replicator ! Update(Counter1Key, PNCounter(), WriteLocal)(_ + 1)
 
val writeTo3 = WriteTo(n = 3, timeout = 1.second)
replicator ! Update(Set1Key, GSet.empty[String], writeTo3)(_ + "hello")
 
val writeMajority = WriteMajority(timeout = 5.seconds)
replicator ! Update(Set2Key, ORSet.empty[String], writeMajority)(_ + "hello")
 
val writeAll = WriteAll(timeout = 5.seconds)
replicator ! Update(ActiveFlagKey, Flag.empty, writeAll)(_.switchOn)
```

作为更新Replicator.UpdateSuccess的响应，如果根据提供的超时中提供的一致性级别成功复制该值，则会将该副本发送到更新的发件人。否则，将发送Replicator.UpdateFailure子类。请注意，Replicator.UpdateTimeout回复并不意味着更新完全失败或回滚。它仍然可以被复制到一些节点，并且最终将被复制到具有gossip协议的所有节点。

```scala
case UpdateSuccess（Counter1Key，req）=> //确定
```

```scala
case UpdateSuccess(Set1Key, req)  => // ok
case UpdateTimeout(Set1Key, req)  =>
// write to 3 nodes failed within 1.second
```
你总是会看到自己的写作。例如，如果您发送两个更改消息更改同一个键的值，第二个消息的修改功能将看到第一个更新消息执行的更改。

在更新消息中，您可以传递一个可选的请求上下文，Replicator不在乎，但包含在回复消息中。这是传递上下文信息（例如原始发送者）而不必使用询问或维持局部相关数据结构的方便方式。

```scala
implicit val node = Cluster(system)
val replicator = DistributedData(system).replicator
val writeTwo = WriteTo(n = 2, timeout = 3.second)
val Counter1Key = PNCounterKey("counter1")
 
def receive: Receive = {
  case "increment" =>
    // incoming command to increase the counter
    val upd = Update(Counter1Key, PNCounter(), writeTwo, request = Some(sender()))(_ + 1)
    replicator ! upd
 
  case UpdateSuccess(Counter1Key, Some(replyTo: ActorRef)) =>
    replyTo ! "ack"
  case UpdateTimeout(Counter1Key, Some(replyTo: ActorRef)) =>
    replyTo ! "nack"
}
```

### Get

要检索数据的当前值，请将Replicator.Get消息发送到Replicator。您提供的一致性级别具有以下含义：

•ReadLocal只会从本地副本中读取该值

•ReadFrom（n）将从n个副本（包括本地副本）读取和合并该值

•ReadMajority该值将从大多数副本（即至少N / 2 + 1个副本）读取和合并，其中N是集群（或集群角色组）中的节点数，

•ReadAll值将从群集中的所有节点（或群集角色组中的所有节点）读取和合并，

```scala
val replicator = DistributedData(system).replicator
val Counter1Key = PNCounterKey("counter1")
val Set1Key = GSetKey[String]("set1")
val Set2Key = ORSetKey[String]("set2")
val ActiveFlagKey = FlagKey("active")
 
replicator ! Get(Counter1Key, ReadLocal)
 
val readFrom3 = ReadFrom(n = 3, timeout = 1.second)
replicator ! Get(Set1Key, readFrom3)
 
val readMajority = ReadMajority(timeout = 5.seconds)
replicator ! Get(Set2Key, readMajority)
 
val readAll = ReadAll(timeout = 5.seconds)
replicator ! Get(ActiveFlagKey, readAll)
```

如果在提供的超时内根据提供的一致性级别成功检索到值，则Get Get Replicator.GetSuccess的响应被发送到Get的发送方。否则将发送Replicator.GetFailure。如果键不存在，答复将是Replicator.NotFound。

```scala
case g @ GetSuccess(Counter1Key, req) =>
  val value = g.get(Counter1Key).value
case NotFound(Counter1Key, req) => // key counter1 does not exist
```

```scala
case g @ GetSuccess(Set1Key, req) =>
  val elements = g.get(Set1Key).elements
case GetFailure(Set1Key, req) =>
// read from 3 nodes failed within 1.second
case NotFound(Set1Key, req)   => // key set1 does not exist
```

你总是会读你自己的写作。例如，如果您发送一个Update消息，然后获取相同的密钥，Get将检索由前面的Update消息执行的更改。然而，未定义应答消息的顺序，即在前面的示例中，您可以在UpdateSuccess之前接收GetSuccess。

在Get消息中，您可以按照与上述Update消息相同的方式传递可选的请求上下文。例如，在接收和变换GetSuccess之后，可以传递和回复原始发送者。

```scala
implicit val node = Cluster(system)
val replicator = DistributedData(system).replicator
val readTwo = ReadFrom(n = 2, timeout = 3.second)
val Counter1Key = PNCounterKey("counter1")
 
def receive: Receive = {
  case "get-count" =>
    // incoming request to retrieve current value of the counter
    replicator ! Get(Counter1Key, readTwo, request = Some(sender()))
 
  case g @ GetSuccess(Counter1Key, Some(replyTo: ActorRef)) =>
    val value = g.get(Counter1Key).value.longValue
    replyTo ! value
  case GetFailure(Counter1Key, Some(replyTo: ActorRef)) =>
    replyTo ! -1L
  case NotFound(Counter1Key, Some(replyTo: ActorRef)) =>
    replyTo ! 0L
}
```

### 一致性

Update和Get中提供的一致性级别根据请求指定必须成功响应写入和读取请求的副本数。对于低延迟读取，您使用ReadLocal具有检索过时数据的风险，即来自其他节点的更新可能不可见。当使用WriteLocal时，更新仅写入本地副本，然后在后台使用gossip协议传播，这可能需要几秒钟才能传播到所有节点。 WriteAll和ReadAll是最强的一致性级别，但也是最慢和最低可用性。例如，一个节点对于Get请求不可用就足够了，您将不会收到该值。如果一致性很重要，您可以使用以下公式确保读取始终反映最近的写入：

（nodes_written + nodes_read）> N

其中N是集群中节点的总数，或者具有用于Replicator的角色的节点数。例如，在7节点集群中，这些一致性属性通过写入4个节点并从4个节点读取，或写入5个节点并从3个节点读取来实现。通过组合WriteMajority和ReadMajority级别，读取总是反映最近的写入。复制器写入和读取大多数副本，即N / 2 + 1.例如，在5节点集群中，它写入3个节点并从3个节点读取。在6节点集群中，它写入4个节点并从4个节点读取。下面是使用WriteMajority和ReadMajority的示例：

```scala
private val timeout = 3.seconds 
private val readMajority = ReadMajority（timeout）
private val writeMajority = WriteMajority（timeout）
```

```scala
def receiveGetCart: Receive = {
  case GetCart =>
    replicator ! Get(DataKey, readMajority, Some(sender()))
 
  case g @ GetSuccess(DataKey, Some(replyTo: ActorRef)) =>
    val data = g.get(DataKey)
    val cart = Cart(data.entries.values.toSet)
    replyTo ! cart
 
  case NotFound(DataKey, Some(replyTo: ActorRef)) =>
    replyTo ! Cart(Set.empty)
 
  case GetFailure(DataKey, Some(replyTo: ActorRef)) =>
    // ReadMajority failure, try again with local read
    replicator ! Get(DataKey, ReadLocal, Some(replyTo))
}
```

```scala
def receiveAddItem: Receive = {
  case cmd @ AddItem(item) =>
    val update = Update(DataKey, LWWMap.empty[LineItem], writeMajority, Some(cmd)) {
      cart => updateCart(cart, item)
    }
    replicator ! update
}
```

在少数情况下，在执行更新时，需要首先尝试从其他节点获取最新数据。这可以通过首先发送具有ReadMajority的Get来完成，然后当接收到GetSuccess，GetFailure或NotFound应答时继续Update。当您需要根据最新信息做出决策或从ORSet或ORMap中删除条目时，可能需要这样做。如果一个条目从一个节点添加到ORSet或ORMap并从另一个节点删除，则只有在添加的条目在执行删除的节点上可见（因此名称为observe-removed set）时，该条目才会被删除。

以下示例说明了如何执行此操作：

```scala
def receiveRemoveItem: Receive = {
  case cmd @ RemoveItem(productId) =>
    // Try to fetch latest from a majority of nodes first, since ORMap
    // remove must have seen the item to be able to remove it.
    replicator ! Get(DataKey, readMajority, Some(cmd))
 
  case GetSuccess(DataKey, Some(RemoveItem(productId))) =>
    replicator ! Update(DataKey, LWWMap(), writeMajority, None) {
      _ - productId
    }
 
  case GetFailure(DataKey, Some(RemoveItem(productId))) =>
    // ReadMajority failed, fall back to best effort local value
    replicator ! Update(DataKey, LWWMap(), writeMajority, None) {
      _ - productId
    }
 
  case NotFound(DataKey, Some(RemoveItem(productId))) =>
  // nothing to remove
}
```

>警告：即使您使用WriteMajority和ReadMajority，如果群集成员资格在更新和获取之间更改，您可能会读取过时的数据的风​​险很小。例如，在5个节点的集群中，当您更新并且该更改写入3个节点：n1，n2，n3。然后添加2个节点，并且Get请求从4个节点读取，这恰好是n4，n5，n6，n7，即，在Get请求的响应中没有看到n1，n2，n3上的值。

### 订阅

您还可以通过向Replicator发送Replicator.Subscribe消息来注册对更改通知的兴趣。当订阅密钥的数据更新时，它将向注册的订阅者发送Replicator.Changed消息。将使用配置的notify-subscriber-interval定期通知订户，并且也可以向Replicator发送显式Replicator.FlushChanges消息，以立即通知订户。

如果用户终止，则用户自动删除。还可以使用Replicator.Uunsubscribe消息取消注册订户。

```scala
val replicator = DistributedData(system).replicator
val Counter1Key = PNCounterKey("counter1")
// subscribe to changes of the Counter1Key value
replicator ! Subscribe(Counter1Key, self)
var currentValue = BigInt(0)
 
def receive: Receive = {
  case c @ Changed(Counter1Key) =>
    currentValue = c.get(Counter1Key).value
  case "get-count" =>
    // incoming request to retrieve current value of the counter
    sender() ! currentValue
}
```

### 删除

可以通过向本地本地复制器发送Replicator.Delete消息来删除数据条目。由于Delete a Replicator.DeleteSuccess的回复被发送到Delete的发送者，如果根据提供的超时时间内提供的一致性级别成功删除了值。否则将发送Replicator.ReplicationDeleteFailure。请注意，ReplicationDeleteFailure并不意味着删除完全失败或已回滚。它仍然可以被复制到一些节点，并且可以最终被复制到所有节点。

已删除的密钥不能再次重复使用，但仍建议删除未使用的数据条目，因为这样可以减少新节点加入群集时的复制开销。随后的Delete，Update和Get请求将使用Replicator.DataDeleted进行回复。订阅者将收到Replicator.DataDeleted。

```scala
val replicator = DistributedData(system).replicator
val Counter1Key = PNCounterKey("counter1")
val Set2Key = ORSetKey[String]("set2")
 
replicator ! Delete(Counter1Key, WriteLocal)
 
val writeMajority = WriteMajority(timeout = 5.seconds)
replicator ! Delete(Set2Key, writeMajority)
```

## 5.8.2数据类型

数据类型必须是收敛（有状态）CRDT并实现ReplicatedData trait，即它们提供单调合并函数，并且状态变化总是收敛。

您可以使用自己的自定义ReplicatedData类型，并且此程序包提供了多种类型，例如：

* 计数器：GCounter，PNCounter

* 集：GSet，ORSet

* 映射：ORMap，ORMultiMap，LWWMap，PNCounterMap
* 寄存器：LWWRegister，Flag

### 	计数

GCounter是一个“只增长计数器”。它只支持增量，没有递减。它的工作方式与向量时钟相似。它跟踪每个节点一个计数器，总值是这些计数器的总和。通过对每个节点取最大计数来实现合并。

如果您需要增量和减量，您可以使用PNCounter（正/负计数器）。

它跟踪与递减（N）分开的增量（P）。 P和N都表示为两个内部GCounter。通过合并内部P和N计数器来处理合并。计数器的值是P计数器的值减去N计数器的值。

hidden val node = Cluster（system）val c0 = PNCounter.empty val c1 = c0 + 1 val c2 = c1 + 7 val c3：PNCounter = c2-2 println（c3.value）// 6

可以使用PNCounterMap数据类型在映射中管理多个相关计数器。当将计数器放置在PNCounterMap中而不​​是将它们作为单独的顶级值时，它们被保证作为一个单元被复制，这有时对于相关数据是必要的。

隐含val节点=集群（系统）val m0 = PNCounterMap.empty val m1 = m0.increment（“a”，7）val m2 = m1.decrement（“a”，2）val m3 = m2.increment ，1）println（m3.get（“a”））// 5 m3.entries.foreach {case（key，value）=> println（s“$ key  - > $ value”）}

### 集合

如果只需要向一个集合添加元素，而不是删除元素，则GSet（仅限成长集）是要使用的数据类型。元素可以是可以序列化的任何类型的值。合并只是两个集合的联合。

val s0 = GSet.empty [String] val s1 = s0 +“a”val s2 = s1 +“b”+“c”if（s2.contains（“a”））println（s2.elements） b，c

如果你需要添加和删除操作，你应该使用ORSet（observe-remove set）。元素可以添加和删除任意次数。如果同时添加和删除元素，则添加将获胜。您不能删除您没有看到的元素。

ORSet有一个版本向量，当一个元素添加到集合中时，该向量会递增。添加元素的节点的版本也在所谓的“出生点”中的每个元素进行跟踪。合并函数使用版本向量和点来跟踪操作的因果性并解决并发更新。

隐含val节点=集群（系统）val s0 = ORSet.empty [String] val s1 = s0 +“a”val s2 = s1 +“b”val s3 = s2  - “a”println（s3.elements）// b

### Map

ORMap（obser-remove map）是一个带有String键的映射，其值是ReplicatedData类型。它支持为地图条目添加，删除和删除任意次数。

如果同时添加和删除条目，则添加将获胜。您无法删除尚未看到的条目。这是与ORSet相同的语义。

如果条目同时更新为不同的值，那么将合并这些值，因此需要这些值必须是ReplicatedData类型。

直接使用ORMap是不方便的，因为它不会暴露特定类型的值。 ORMap旨在作为构建更具体的地图的低级工具，例如以下专门化地图。

ORMultiMap（观察 - 删除多图）是一个多地图实现，它将ORMap与地图值的ORSet包起来。

PNCounterMap（正负计数器映射）是命名计数器的映射。它是一个带有PNCounter值的专门的ORMap。

LWWMap（最后一个作家赢得地图）是一个专门的ORMap与LWWRegister（最后一个作家赢得寄存器）值。

隐含val节点=集群（系统）val m0 = ORMultiMap.empty [Int] val m1 = m0 +（“a”→Set（1,2,3））val m2 = m1.addBinding（“a”，4） val m3 = m2.removeBinding（“a”，2）

＃unreachable，因此应该配置为在健康集群中的最坏情况。最大修剪传播= 60秒

＃Serialized Write和Read消息在发送到＃个节点时被缓存。如果没有进一步的活动，它们将在此持续时间之后从高速缓存＃中移除。 serializer-cache-time-to-live = 10s

}}


Flags and Registers
Flag is a data type for a boolean value that is initialized to false and can be switched to true. Thereafter it cannot be changed. true wins over false in merge.

val f0 = Flag.empty
val f1 = f0.switchOn
println(f1.enabled)
LWWRegister (last writer wins register) can hold any (serializable) value.

Merge of a LWWRegister takes the register with highest timestamp. Note that this relies on synchronized clocks. LWWRegister should only be used when the choice of value is not important for concurrent updates occurring within the clock skew.

Merge takes the register updated by the node with lowest address (UniqueAddress is ordered) if the timestamps are exactly the same.

implicit val node = Cluster(system)
val r1 = LWWRegister("Hello")
val r2 = r1.withValue("Hi")
println(s"${r1.value} by ${r1.updatedBy} at ${r1.timestamp}")
Instead of using timestamps based on System.currentTimeMillis() time it is possible to use a timestamp value based on something else, for example an increasing version number from a database record that is used for optimistic concurrency control.

case class Record(version: Int, name: String, address: String)
 
implicit val node = Cluster(system)
implicit val recordClock = new LWWRegister.Clock[Record] {
  override def apply(currentTimestamp: Long, value: Record): Long =
    value.version
}
 
val record1 = Record(version = 1, "Alice", "Union Square")
val r1 = LWWRegister(record1)
 
val record2 = Record(version = 2, "Alice", "Madison Square")
val r2 = LWWRegister(record2)
 
val r3 = r1.merge(r2)
println(r3.value)
For first-write-wins semantics you can use the LWWRegister#reverseClock instead of the LWWRegister#defaultClock.

The defaultClock is using max value of System.currentTimeMillis() and currentTimestamp + 1. This means that the timestamp is increased for changes on the same node that occurs within the same millisecond. It also means that it is safe to use the LWWRegister without synchronized clocks when there is only one active writer, e.g. a Cluster Singleton. Such a single writer should then first read current value with ReadMajority (or more) before changing and writing the value with WriteMajority (or more).

Custom Data Type
You can rather easily implement your own data types. The only requirement is that it implements the merge function of the ReplicatedData trait.

A nice property of stateful CRDTs is that they typically compose nicely, i.e. you can combine several smaller data types to build richer data structures. For example, the PNCounter is composed of two internal GCounter instances to keep track of increments and decrements separately.

Here is s simple implementation of a custom TwoPhaseSet that is using two internal GSet types to keep track of addition and removals. A TwoPhaseSet is a set where an element may be added and removed, but never added again thereafter.

case class TwoPhaseSet(
  adds:     GSet[String] = GSet.empty,
  removals: GSet[String] = GSet.empty)
  extends ReplicatedData {
  type T = TwoPhaseSet
 
  def add(element: String): TwoPhaseSet =
    copy(adds = adds.add(element))
 
  def remove(element: String): TwoPhaseSet =
    copy(removals = removals.add(element))
 
  def elements: Set[String] = adds.elements diff removals.elements
 
  override def merge(that: TwoPhaseSet): TwoPhaseSet =
    copy(
      adds = this.adds.merge(that.adds),
      removals = this.removals.merge(that.removals))
}
Data types should be immutable, i.e. "modifying" methods should return a new instance.

Serialization
The data types must be serializable with an Akka Serializer. It is highly recommended that you implement efficient serialization with Protobuf or similar for your custom data types. The built in data types are marked with ReplicatedDataSerialization and serialized with akka.cluster.ddata.protobuf.ReplicatedDataSerializer.

Serialization of the data types are used in remote messages and also for creating message digests (SHA-1) to detect changes. Therefore it is important that the serialization is efficient and produce the same bytes for the same content. For example sets and maps should be sorted deterministically in the serialization.

This is a protobuf representation of the above TwoPhaseSet:

option java_package = "docs.ddata.protobuf.msg";
option optimize_for = SPEED;
 
message TwoPhaseSet {
  repeated string adds = 1;
  repeated string removals = 2;
}
The serializer for the TwoPhaseSet:

```scala
import java.util.ArrayList
import java.util.Collections
import scala.collection.JavaConverters._
import akka.actor.ExtendedActorSystem
import akka.cluster.ddata.GSet
import akka.cluster.ddata.protobuf.SerializationSupport
import akka.serialization.Serializer
import docs.ddata.TwoPhaseSet
import docs.ddata.protobuf.msg.TwoPhaseSetMessages
 
class TwoPhaseSetSerializer(val system: ExtendedActorSystem)
  extends Serializer with SerializationSupport {
 
  override def includeManifest: Boolean = false
 
  override def identifier = 99999
 
  override def toBinary(obj: AnyRef): Array[Byte] = obj match {
    case m: TwoPhaseSet => twoPhaseSetToProto(m).toByteArray
    case _ => throw new IllegalArgumentException(
      s"Can't serialize object of type ${obj.getClass}")
  }
 
  override def fromBinary(bytes: Array[Byte], clazz: Option[Class[_]]): AnyRef = {
    twoPhaseSetFromBinary(bytes)
  }
 
  def twoPhaseSetToProto(twoPhaseSet: TwoPhaseSet): TwoPhaseSetMessages.TwoPhaseSet = {
    val b = TwoPhaseSetMessages.TwoPhaseSet.newBuilder()
    // using java collections and sorting for performance (avoid conversions)
    val adds = new ArrayList[String]
    twoPhaseSet.adds.elements.foreach(adds.add)
    if (!adds.isEmpty) {
      Collections.sort(adds)
      b.addAllAdds(adds)
    }
    val removals = new ArrayList[String]
    twoPhaseSet.removals.elements.foreach(removals.add)
    if (!removals.isEmpty) {
      Collections.sort(removals)
      b.addAllRemovals(removals)
    }
    b.build()
  }
 
  def twoPhaseSetFromBinary(bytes: Array[Byte]): TwoPhaseSet = {
    val msg = TwoPhaseSetMessages.TwoPhaseSet.parseFrom(bytes)
    TwoPhaseSet(
      adds = GSet(msg.getAddsList.iterator.asScala.toSet),
      removals = GSet(msg.getRemovalsList.iterator.asScala.toSet))
  }
}
```

Note that the elements of the sets are sorted so the SHA-1 digests are the same for the same elements.

You register the serializer in configuration:
```
akka.actor {
  serializers {
    two-phase-set = "docs.ddata.protobuf.TwoPhaseSetSerializer"
  }
  serialization-bindings {
    "docs.ddata.TwoPhaseSet" = two-phase-set
  }
}
```
Using compression can sometimes be a good idea to reduce the data size. Gzip compression is provided by the akka.cluster.ddata.protobuf.SerializationSupport trait:

```
override def toBinary(obj: AnyRef): Array[Byte] = obj match {
  case m: TwoPhaseSet => compress(twoPhaseSetToProto(m))
  case _ => throw new IllegalArgumentException(
    s"Can't serialize object of type ${obj.getClass}")
}
 
override def fromBinary(bytes: Array[Byte], clazz: Option[Class[_]]): AnyRef = {
  twoPhaseSetFromBinary(decompress(bytes))
}
```

The two embedded GSet can be serialized as illustrated above, but in general when composing new data types from the existing built in types it is better to make use of the existing serializer for those types. This can be done by declaring those as bytes fields in protobuf:

message TwoPhaseSet2 {
  optional bytes adds = 1;
  optional bytes removals = 2;
}
and use the methods otherMessageToProto and otherMessageFromBinary that are provided by the SerializationSupport trait to serialize and deserialize the GSet instances. This works with any type that has a registered Akka serializer. This is how such an serializer would look like for the TwoPhaseSet:

```scala
import akka.actor.ExtendedActorSystem
import akka.cluster.ddata.GSet
import akka.cluster.ddata.protobuf.ReplicatedDataSerializer
import akka.cluster.ddata.protobuf.SerializationSupport
import akka.serialization.Serializer
import docs.ddata.TwoPhaseSet
import docs.ddata.protobuf.msg.TwoPhaseSetMessages
 
class TwoPhaseSetSerializer2(val system: ExtendedActorSystem)
  extends Serializer with SerializationSupport {
 
  override def includeManifest: Boolean = false
 
  override def identifier = 99999
 
  val replicatedDataSerializer = new ReplicatedDataSerializer(system)
 
  override def toBinary(obj: AnyRef): Array[Byte] = obj match {
    case m: TwoPhaseSet => twoPhaseSetToProto(m).toByteArray
    case _ => throw new IllegalArgumentException(
      s"Can't serialize object of type ${obj.getClass}")
  }
 
  override def fromBinary(bytes: Array[Byte], clazz: Option[Class[_]]): AnyRef = {
    twoPhaseSetFromBinary(bytes)
  }
 
  def twoPhaseSetToProto(twoPhaseSet: TwoPhaseSet): TwoPhaseSetMessages.TwoPhaseSet2 = {
    val b = TwoPhaseSetMessages.TwoPhaseSet2.newBuilder()
    if (!twoPhaseSet.adds.isEmpty)
      b.setAdds(otherMessageToProto(twoPhaseSet.adds).toByteString())
    if (!twoPhaseSet.removals.isEmpty)
      b.setRemovals(otherMessageToProto(twoPhaseSet.removals).toByteString())
    b.build()
  }
 
  def twoPhaseSetFromBinary(bytes: Array[Byte]): TwoPhaseSet = {
    val msg = TwoPhaseSetMessages.TwoPhaseSet2.parseFrom(bytes)
    val adds =
      if (msg.hasAdds)
        otherMessageFromBinary(msg.getAdds.toByteArray).asInstanceOf[GSet[String]]
      else
        GSet.empty[String]
    val removals =
      if (msg.hasRemovals)
        otherMessageFromBinary(msg.getRemovals.toByteArray).asInstanceOf[GSet[String]]
      else
        GSet.empty[String]
    TwoPhaseSet(adds, removals)
  }
}
```

CRDT Garbage
One thing that can be problematic with CRDTs is that some data types accumulate history (garbage). For example a GCounter keeps track of one counter per node. If a GCounter has been updated from one node it will associate the identifier of that node forever. That can become a problem for long running systems with many cluster nodes being added and removed. To solve this problem the Replicator performs pruning of data associated with nodes that have been removed from the cluster. Data types that need pruning have to implement the RemovedNodePruning trait.

Samples
Several interesting samples are included and described in the Lightbend Activator tutorial named Akka Distributed Data Samples with Scala.

Low Latency Voting Service
Highly Available Shopping Cart
Distributed Service Registry
Replicated Cache
Replicated Metrics
Limitations
There are some limitations that you should be aware of.

CRDTs cannot be used for all types of problems, and eventual consistency does not fit all domains. Sometimes you need strong consistency.

It is not intended for Big Data. The number of top level entries should not exceed 100000. When a new node is added to the cluster all these entries are transferred (gossiped) to the new node. The entries are split up in chunks and all existing nodes collaborate in the gossip, but it will take a while (tens of seconds) to transfer all entries and this means that you cannot have too many top level entries. The current recommended limit is 100000. We will be able to improve this if needed, but the design is still not intended for billions of entries.

All data is held in memory, which is another reason why it is not intended for Big Data.

When a data entry is changed the full state of that entry is replicated to other nodes. For example, if you add one element to a Set with 100 existing elements, all 101 elements are transferred to other nodes. This means that you cannot have too large data entries, because then the remote message size will be too large. We might be able to make this more efficient by implementing Efficient State-based CRDTs by Delta-Mutation.

The data is only kept in memory. It is redundant since it is replicated to other nodes in the cluster, but if you stop all nodes the data is lost, unless you have saved it elsewhere. Making the data durable is a possible future feature, but even if we implement that it is not intended to be a full featured database.

Learn More about CRDTs
The Final Causal Frontier talk by Sean Cribbs
Eventually Consistent Data Structures talk by Sean Cribbs
Strong Eventual Consistency and Conflict-free Replicated Data Types talk by Mark Shapiro
A comprehensive study of Convergent and Commutative Replicated Data Types paper by Mark Shapiro et. al.
Dependencies
To use Distributed Data you must add the following dependency in your project.

sbt:

```
"com.typesafe.akka" %% "akka-distributed-data-experimental" % "2.4.14"
```

maven:

```
<dependency>
  <groupId>com.typesafe.akka</groupId>
  <artifactId>akka-distributed-data-experimental_2.11</artifactId>
  <version>2.4.14</version>
</dependency>
```

Configuration
The DistributedData extension can be configured with the following properties:

```
# Settings for the DistributedData extension
akka.cluster.distributed-data {
  # Actor name of the Replicator actor, /system/ddataReplicator
  name = ddataReplicator
 
  # Replicas are running on members tagged with this role.
  # All members are used if undefined or empty.
  role = ""
 
  # How often the Replicator should send out gossip information
  gossip-interval = 2 s
 
  # How often the subscribers will be notified of changes, if any
  notify-subscribers-interval = 500 ms
 
  # Maximum number of entries to transfer in one gossip message when synchronizing
  # the replicas. Next chunk will be transferred in next round of gossip.
  max-delta-elements = 1000
  
  # The id of the dispatcher to use for Replicator actors. If not specified
  # default dispatcher is used.
  # If specified you need to define the settings of the actual dispatcher.
  use-dispatcher = ""
 
  # How often the Replicator checks for pruning of data associated with
  # removed cluster nodes.
  pruning-interval = 30 s
  
  # How long time it takes (worst case) to spread the data to all other replica nodes.
  # This is used when initiating and completing the pruning process of data associated
  # with removed cluster nodes. The time measurement is stopped when any replica is 
  # unreachable, so it should be configured to worst case in a healthy cluster.
  max-pruning-dissemination = 60 s
  
  # Serialized Write and Read messages are cached when they are sent to 
  # several nodes. If no further activity they are removed from the cache
  # after this duration.
  serializer-cache-time-to-live = 10s
  
}
```