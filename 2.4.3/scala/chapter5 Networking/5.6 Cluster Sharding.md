[TOC]

# 5.6 集群分片

当您需要将actor分散到群集中的多个节点上,并且希望能使用其逻辑标识符与它们进行交互，而不必关心其在群集中的物理位置（这也可能随时间而变化）时，群集分片非常有用。

例如, 它可以是表示域驱动设计术语中的`Aggregate Roots`的角色。这里我们称这些actor为**实体(Entities)**。这些actor通常具有状态持久化的特征，但是这个功能(指分片)不局限于具有状态持久化的actors。

使用集群分片的一个典型场景是, 当您有许多有状态的actor, 而且这些actor消耗的资源（例如内存）超过一台机器的限制的时候。如果你只有几个有状态的actor，在Cluster Singleton节点上运行它们可能会更容易一些。

在本文中，分片意味着具有标识符的actor（即所谓的实体）可以自动分布到集群中的多个节点上。每个实体actor仅在一个地方运行，并且可以向该实体发送消息，而不需要发送方知道目标actor的位置。这是通过由该扩展提供的ShardRegion actor发送消息来实现的，该扩展知道如何将具有实体id的消息路由到最终目的地。

>如果启用了此功能，群集分片将不会在WeaklyUp状态的成员上激活。

## 5.6.1 示例

下面是一个实体actor的例子：

```scala
case object Increment
case object Decrement
final case class Get(counterId: Long)
final case class EntityEnvelope(id: Long, payload: Any)
 
case object Stop
final case class CounterChanged(delta: Int)
 
class Counter extends PersistentActor {
  import ShardRegion.Passivate
 
  context.setReceiveTimeout(120.seconds)
 
  // self.path.name is the entity identifier (utf-8 URL-encoded)
  override def persistenceId: String = "Counter-" + self.path.name
 
  var count = 0
 
  def updateState(event: CounterChanged): Unit =
    count += event.delta
 
  override def receiveRecover: Receive = {
    case evt: CounterChanged ⇒ updateState(evt)
  }
 
  override def receiveCommand: Receive = {
    case Increment      ⇒ persist(CounterChanged(+1))(updateState)
    case Decrement      ⇒ persist(CounterChanged(-1))(updateState)
    case Get(_)         ⇒ sender() ! count
    case ReceiveTimeout ⇒ context.parent ! Passivate(stopMessage = Stop)
    case Stop           ⇒ context.stop(self)
  }
}
```

上面的actor使用Event Sourceing和PersistentActor中提供的支持来存储它的状态。它不必是一个持久的actor，但是在节点之间的实体失败或迁移的情况下，它必须能够恢复其状态，如果它是有价值的。

注意如何定义persistenceId。 actor的名称是实体标识符（utf-8 URL编码）。你可以用另一种方式定义它，但它必须是唯一的。

当使用分片扩展时，首先，通常是在集群中每个节点上的系统启动时，应使用ClusterSharding.start方法注册支持的实体类型。 ClusterSharding.start为您提供了可以传递的引用。

```scala
val counterRegion: ActorRef = ClusterSharding(system).start(
  typeName = "Counter",
  entityProps = Props[Counter],
  settings = ClusterShardingSettings(system),
  extractEntityId = extractEntityId,
  extractShardId = extractShardId)
```

extractEntityId和extractShardId是两个应用程序特定的函数，用于从传入消息中提取实体标识符和分片标识符。

```scala
val extractEntityId: ShardRegion.ExtractEntityId = {
  case EntityEnvelope(id, payload) ⇒ (id.toString, payload)
  case msg @ Get(id)               ⇒ (id.toString, msg)
}
 
val numberOfShards = 100
 
val extractShardId: ShardRegion.ExtractShardId = {
  case EntityEnvelope(id, _) ⇒ (id % numberOfShards).toString
  case Get(id)               ⇒ (id % numberOfShards).toString
}
```

此示例说明了在消息中定义实体标识符的两种不同方法：

>* Get消息包括标识符本身。
>* EntityEnvelope保存标识符，发送到实体actor的实际消息被包装在信封中。

请注意如何在上面显示的extractEntityId函数中处理这两种消息类型。发送到实体actor的消息是由extractEntityId返回的元组的第二部分，并且使得有可能在需要时解包信封。

分片是一组将被一起管理的实体。分组由上述extractShardId函数定义。对于特定实体标识符，分片标识符必须始终相同。

创建一个好的分片算法本身就是一个有趣的挑战。尝试产生均匀分布，即每个分片中的实体数量相同。作为经验法则，分片数量应该是大于计划的群集节点最大数量的10倍。少于节点数的分片将导致一些节点不托管任何分片。太多的分片将导致对分片的低效管理，例如。重新平衡开销，增加延迟，因为协调器参与每个分片的第一个消息的路由。分片算法必须在正在运行的集群中的所有节点上相同。它可以在停止集群中的所有节点后更改。

在大多数情况下工作正常的简单分片算法是将实体标识符的hashCode的绝对值乘以分片数。为方便起见，这是由ShardRegion.HashCodeMessageExtractor提供的。

到实体的消息总是通过本地ShardRegion发送。命名实体类型的ShardRegion actor引用由ClusterSharding.start返回，也可以使用ClusterSharding.shardRegion检索。 ShardRegion将查找实体的分片的位置，如果它还不知道它的位置。它将把消息委托给正确的节点，并且它将按需创建实体动作者，即当传送用于特定实体的第一消息时。

```scala
val counterRegion: ActorRef = ClusterSharding(system).shardRegion("Counter")
counterRegion ! Get(123)
expectMsg(0)
 
counterRegion ! EntityEnvelope(123, Increment)
counterRegion ! Get(123)
expectMsg(1)
```

在Typesafe Activator教程中，有一个更全面的示例，名为`Akka Cluster Sharding with Scala!`.

## 5.6.2 如何工作

ShardRegion actor在集群中的每个节点或标记有特定角色的节点组中启动。 ShardRegion有两个特定的功能，从传入的消息中提取实体标识符以及分片标识符。**分片是将被一起管理的一组实体**。当接收到特定分片的第一条消息时，ShardRegion将会请求中央协调器`ShardCoordinator`以获取分片的位置。

`ShardCoordinator`决定哪个ShardRegion将拥有Shard, 并通知ShardRegion。该region将确认此请求并创建Shard主管作为子actor。然后将由分片执行者在需要时创建各个实体。因此，传入的消息通过ShardRegion和Shard传递到目标实体。

如果分片首页是另一个ShardRegion实例，则消息将被转发到该ShardRegion实例。在解析分片的分片传入消息的位置时，缓冲并稍后在已知分片首页时传递。解析的分片的后续消息可以立即传递到目标目标，而不涉及ShardCoordinator。

情况1：

1. 传入消息M1到ShardRegion实例R1。

2. M1映射到分片S1。 R1不知道S1，因此它向协调器C询问S1的位置。

3. C回答S1的家是R1。

4. R1为实体E1创建子actor，并将S1的缓存消息发送给E1子实体

5. 到达R1的所有消息S1可以由没有C的R1处理。它根据需要创建实体子节点，并向它们转发消息。

情形2：

1. 将消息M2传送到R1。

2. M2被映射到S2。 R1不知道S2，所以它要求C找到S2的位置。

3. C回答S2的家是R2。

4. R1将S2的缓冲消息发送到R2

5. 到达R1的所有进入消息S2可以由没有C的R1处理。它将消息转发到R2。

R2接收S2的消息，请求C，它回答S2的归属是R2，我们在场景1（但对于R2）。

为了确保特定实体actor的至少一个实例正在集群中的某处运行，要保证所有节点都知道所有分片所在位置的相同视图。因此，分片的分配决定是由以集群单粒模式运行的中心ShardCoordinator来负责的，即在所有集群节点中的最早成员上的一个实例或者用特定角色标记的一组节点。

在可插入分片分配策略中定义了决定分片位于何处的逻辑。默认实现ShardCoordinator.LeastShardAllocationStrategy使用最少数量的先前分配的分片向ShardRegion分配新的分片。此策略可以由应用程序特定实现来替代。

为了能够在集群中使用新添加的成员，协调器促进分片的重新平衡，即将实体从一个节点迁移到另一个节点。在重新平衡过程中，协调器首先通知所有ShardRegion actor一个分片的切换已经开始。这意味着它们将开始缓冲该分片的入站消息，就像分片位置未知一样。在重新平衡过程期间，协调器将不回答对正被重新平衡的分片的位置的任何请求，即本地缓冲将继续，直到切换完成。负责重新平衡shard的ShardRegion将通过向其发送指定的handOffStopMessage（默认PoisonPill）来停止该分片中的所有实体。当所有实体被终止时，拥有实体的ShardRegion将确认切换完成到协调器。此后，协调器将回复对分片位置的请求，从而为分片分配新的主区，然后将ShardRegion actors中的缓冲消息传送到新位置。这意味着实体的状态不会被传输或迁移。如果实体的状态是重要的，它应该是持久的（持久的），例如。与持久性，以便它可以在新位置恢复。

决定哪些分片重新平衡的逻辑在可插入分片分配策略中定义。缺省实现ShardCoordinator.LeastShardAllocationStrategy从具有最多数量的先前分配的分片的ShardRegion中选择用于切换的分片。然后，它们将被分配给具有最少数量的先前分配的分片的ShardRegion，即集群中的新成员。有一个可配置的阈值，差异必须有多大才能开始重新平衡。此策略可以由应用程序特定实现来替代。

ShardCoordinator中的分片位置的状态是持久的（持久的），具有持久性以在故障中生存。由于它在集群中运行，持久性必须使用分布式日志配置。当崩溃或无法到达的协调器节点已从集群中删除（通过向下）新的ShardCoordinator


单独的actor将接管并且状态被恢复。在这样的故障期间，具有已知位置的分片仍然可用，而用于新（未知）分片的消息被缓冲，直到新的ShardCoordinator变得可用。

只要发送方使用相同的ShardRegion actor将消息传递给实体actor，消息的顺序就会被保留。只要没有达到缓冲区限制，以最大努力为基础，以最多一次递送语义，以与普通消息发送相同的方式递送消息。可以通过在持久性中使用AtLeastOnceDelivery来添加可靠的端到端消息传递，具有至少一次语义。

由于到协调器的往返，对于针对新的或先前未使用的分片的消息引入了一些附加的等待时间。分片的重新平衡也可能增加延迟。在设计应用程序特定的分片分辨率时应该考虑这一点。以避免过细粒度分片。

## 5.6.3分布式数据模式

不使用持久性，可以使用分布式数据模块作为分片协调器状态的存储。在这种情况下，ShardCoordinator的状态将由具有WriteMajority / ReadMajority一致性的Distributed Data模块在集群内复制。

可以通过将配置属性akka.cluster.sharding.state-store-mode设置为ddata来启用此模式。

如果使用此模式，则必须将akka-distributed-data-experimental依赖项显式添加到构建中。如果没有在用户代码中使用并且记住实体关闭，那么可以从项目中删除akka-persistence依赖项。

警告：在Akka 2.4.0中引入ddata模式被认为是"实验性"模式，因为它取决于实验性分布式数据模块。

## 5.6.4 仅代理模式

ShardRegion actor也可以在仅代理模式下启动，即它不会托管任何实体本身，但知道如何将消息委派到正确的位置。 ShardRegion在仅代理模式下使用方法ClusterSharding.startProxy方法启动。

## 5.6.5 钝化

如果实体的状态是持久的，则可以停止不用于减少内存消耗的实体。这通过实体actors的应用程序特定实现来完成，例如通过定义接收超时（context.setReceiveTimeout）。如果消息已经排队到实体，当它自己停止时，邮箱中的排队消息将被删除。为了支持优雅的钝化而不丢失这样的消息，实体actor可以向其父Shard发送ShardRegion.Passivate。在Passivate中指定的包裹消息将被发送回实体，然后它应该停止自身。传入消息将由Shard在接收Passivate和终止实体之间进行缓冲。这种缓冲的消息此后被传送到实体的新化身。

## 5.6.6 记住实体

通过在调用ClusterSharding.start时在ClusterShardingSettings中将rememberEntities标志设置为true，可以使每个Shard中的实体列表成为持久性（持久）。当配置为记住实体时，每当Shard重新平衡到另一个节点或在崩溃后恢复时，它将重新创建先前在该Shard中运行的所有实体。要永久停止实体，必须将钝化消息发送到实体actor的父实体，否则实体将在配置中指定的实体重新启动后退之后自动重新启动。


当rememberEntities设置为false时，Shard在重新平衡或从崩溃中恢复后不会自动重新启动任何实体。只有在Shard中收到该实体的第一条消息后，才会启动实体。如果实体在不使用钝化的情况下停止，将不会重新启动。

注意，除非它们被持久化，否则实体本身的状态将不会被恢复。与持久。

## 5.6.7 监督

如果您需要为默认（重新启动）策略之外的实体actors使用另一个supervisorStrategy，您需要创建一个中间父actor来定义子实体actor的supervisorStrategy。

```scala
class CounterSupervisor extends Actor {
  val counter = context.actorOf(Props[Counter], "theCounter")
 
  override val supervisorStrategy = OneForOneStrategy() {
    case _: IllegalArgumentException     ⇒ SupervisorStrategy.Resume
    case _: ActorInitializationException ⇒ SupervisorStrategy.Stop
    case _: DeathPactException           ⇒ SupervisorStrategy.Stop
    case _: Exception                    ⇒ SupervisorStrategy.Restart
  }
 
  def receive = {
    case msg ⇒ counter forward msg
  }
}
```

你开始这样的主管，就像是实体的演员一样。

```scala
ClusterSharding(system).start(
  typeName = "SupervisedCounter",
  entityProps = Props[CounterSupervisor],
  settings = ClusterShardingSettings(system),
  extractEntityId = extractEntityId,
  extractShardId = extractShardId)
```

请注意，当新邮件定向到实体时，停止的实体将再次启动。

## 5.6.8正常关机

您可以向ShardRegion actor发送消息ClusterSharding.GracefulShutdown消息以切换由该ShardRegion托管的所有分片，然后ShardRegion actor将停止。您可以观看ShardRegion actor以知道它何时完成。在此期间，其他区域将以与协调器触发重新平衡时相同的方式缓冲那些分片的消息。当分片已停止时，协调器将在其他地方分配这些分片。

当ShardRegion终止时，您可能想离开集群，并关闭ActorSystem。

这是怎么做的：

```scala
class IllustrateGracefulShutdown extends Actor {
  val system = context.system
  val cluster = Cluster(system)
  val region = ClusterSharding(system).shardRegion("Entity")
 
  def receive = {
    case "leave" ⇒
      context.watch(region)
      region ! ShardRegion.GracefulShutdown
 
    case Terminated(`region`) ⇒
      cluster.registerOnMemberRemoved(self ! "member-removed")
      cluster.leave(cluster.selfAddress)
 
    case "member-removed" ⇒
      // Let singletons hand over gracefully before stopping the system
      import context.dispatcher
      system.scheduler.scheduleOnce(10.seconds, self, "stop-system")
 
    case "stop-system" ⇒
      system.terminate()
  }
}
```

## 5.6.9删除内部簇分片数据

集群分片协调器使用Akka Persistence来存储分片的位置。重新启动整个Akka群集时，可以安全地删除此数据。请注意，这不是应用程序数据。

有一个实用程序akka.cluster.sharding.RemoveInternalClusterShardingData删除此数据。

警告：在运行正在使用群集分片的Akka群集节点时，切勿使用此程序。在使用此程序之前停止所有群集节点。

如果Cluster Sharding协调器由于数据损坏而无法启动，则可能需要删除数据，如果意外地两个集群同时运行（例如，导致由自动关闭和有一个网络分区。

将此程序用作独立的Java主程序：

java -classpath <jar文件，包括akka-cluster-sharding> akka.cluster.sharding.RemoveInternalClusterShardingData -2.3 entityType1 entityType2 entityType3

该程序包含在akka-cluster-sharding jar文件中。最简单的方法是使用与普通应用程序相同的类路径和配置。它可以从sbt或maven以类似的方式运行。

指定实体类型名称（与在ClusterSharding的start方法中使用的名称相同）作为程序参数。

如果指定-2.3作为第一个程序参数，它也将尝试使用不同的persistenceId删除由Akka 2.3.x中的Cluster Sharding存储的数据。

## 5.6.10依赖关系

要使用集群分片，必须在项目中添加以下依赖项。

sbt：

```
"com.typesafe.akka"%%"akka-cluster-sharding"％"2.4.1"
```

maven：

```xml
<dependency>
  <groupId>com.typesafe.akka</groupId>
  <artifactId>akka-cluster-sharding_2.11</artifactId>
  <version>2.4.14</version>
</dependency>
```

## 5.6.11 组态

可以使用以下属性配置ClusterSharding扩展。当使用ActorSystem参数创建时，这些配置属性由ClusterShardingSettings读取。还可以修改ClusterShardingSettings或从另一个配置节创建它与下面相同的布局。 ClusterShardingSettings是ClusterSharding扩展的start方法的参数，即，如果需要，每个实体类型都可以使用不同的设置进行配置。

```bash
#ClusterShardingExtension的设置

akka.cluster.sharding {

#该扩展在顶级系统范围中创建具有此名称的顶级actor。 '/ system / sharding'
guardian-name = sharding

#指定实体在具有特定角色的集群节点上运行。 #如果未指定角色（或空），则使用集群中的所有节点。 
role =""

#当这被设置为'on'时，活动实体actors将在Shard重新启动时自动重新启动#。即如果Shard由于重新平衡或崩溃而在不同的ShardRegion#上启动。 
remember-entities = off

#如果协调器无法存储状态更改，则将停止#并在此持续时间后重新启动，指数退避#高达此持续时间的5倍。 
coordinator-failure-backoff = 5 s

#如果ShardRegion没有回复，ShardRegion将以此间隔重试对ShardCoordinator的注册和分片请求。 
retry-interval = 2 s

#ShardRegion actor缓冲的最大消息数。 
buffer-size = 100000

#分片重新平衡过程超时。切换超时= 60秒

#给一个地区承认它承载一个分片的时间。 
shard-start-timeout = 10 s

#如果分片是记住实体，并且不能存储状态更改#将被停止，然后在此持续时间后重新启动。在此过程中，发送到受影响实体的任何消息#可能会丢失。 
shard-failure-backoff = 10 s

#如果分片记住实体，并且实体在没有#使用钝化的情况下停止。实体将在此持续时间之后重新启动，或者当接收到#下一个消息时，将首先发生。 
entity-restart-backoff = 10 s

#使用此间隔定期执行重新平衡检查。 rebalance-interval = 10 s

#日志插件配置实体的绝对路径，用于ClusterSharding的内部持久性。如果未定义#，则使用默认日志插件。注意，这与实体actors使用的#persistence无关。 
journal-plugin-id =""

#快照插件配置实体的绝对路径，用于ClusterSharding的内部持久性。如果未定义#，则使用默认快照插件。注意，这与实体actors使用的#persistence无关。 
snapshot-plugin-id =""

#确定协调器将如何存储状态的参数#有效值"持久性"或"ddata"#"ddata"模式是实验性的，因为它取决于实验#模块akka分布式数据实验。

state-store-mode ="persistence"

#Shard在持续的#个事件之后保存持久性快照。快照用于减少恢复时间。 snapshot-after = 1000

#默认分片策略的设置minimum-shard-allocation-strategy {#分配的最大和最小分片数之间差异必须达到的最大值才能开始重新平衡。重新平衡阈值= 10

#正在进行的重新平衡过程的数量限制为此数量。 
max-simultaneous-rebalance = 3

}

#等待初始分布式状态的超时（如果th#仅用于状态存储模式="ddata"waiting-for-state-timeout = 5秒，则将重新查询初始状态
#等待更新分布式状态的超时（如果超时hap#仅用于状态存储模式="ddata"更新状态超时= 5秒，将重试更新
#协调器单件的设置。与akka.cluster.singleton相同的布局。 
coordinator-singleton = $ {akka.cluster.singleton}

#用于ClusterSharding actors的分派器的ID。 #如果未指定，则使用默认分派器。 
#如果指定，您需要定义实际调度程序的设置。 #实体actors的调度器由用户提供的
#props定义，即此调度器不用于实体actors。 
use-dispatcher =""

}
```
自定义分片分配策略可以在ClusterSharding.start的可选参数中定义。有关如何实现自定义分片分配策略的详细信息，请参阅ShardAllocationStrategy的API文档。

## 5.6.12 检查集群分片状态

检查集群状态的两个请求可用：

`ClusterShard.GetShardRegionState`，它将返回一个`ClusterShard.ShardRegionState`，它包含'Shar-dId在一个区域中运行，以及EntityId对于每个区域都是活着的。

ClusterShard.GetClusterShardingStats，它将查询集群中的所有区域，并返回一个包含ShardId在每个区域中运行的Cluster-Shard.ClusterShardingStats，以及每个分片中活动的实体数。

这些消息的目的是测试和监视，它们不提供访问直接发送消息到各个实体。


